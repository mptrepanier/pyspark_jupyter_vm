{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Spark\n",
    "In order to link with any Spark program, we need to import some Spark classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To communicate we Spark, we need to create a `SparkContext` object. The `SparkContext` object informs Spark on how to access a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Learning Spark\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with RDDs\n",
    "\n",
    "`RDDs`, or *Resilient Distributed Datasets*, are immutable (can't be modified), distributed collections of elements of your data. Prior to Spark 1.6, RDDs were the only API availble for interacting with your data in Spark. While more recent versions introduce the  `DataFrame` and `DataSet` APIs, there's no better place to begin learning Spark than with RDDs.\n",
    "\n",
    "Let's begin our notebook by reading a \".csv\" file into an RDD. The sample data contains sales information from an imaginary stationery company and has the following schema:\n",
    "\n",
    "`|OrderDate|Region|Rep|Item|Units|Unit Cost|Total|`\n",
    "\n",
    "*Sample data courtesy: http://www.contextures.com/xlSampleData01.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'OrderDate,Region,Rep,Item,Units,Unit Cost,Total', u'1/6/2016,East,Jones,Pencil,95,1.99,189.05', u'1/23/2016,Central,Kivell,Binder,50,19.99,999.50']\n"
     ]
    }
   ],
   "source": [
    "raw_rdd = sc.textFile(\"sample_data/stationery.csv\")\n",
    "print raw_rdd.take(3) # take(n) grabs the first n rows of an RDD and returns them in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above printed list, we can see that while we were able to read everything in, but the contents of each row are bunched together in long strings and not separated by their commas. As well, the documents header is being included in the first row of the RDD. This result is due to limitations with the `textFile()` method's functionality. Newer APIs (`DataFrames and DataSets`) have built in fileType resolutions. For now - how do we correct this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions\n",
    "RDDs offer two options for interacting with them: **transformations** and **actions**. **Transformations** construct new RDDs from previous ones and  **actions** compute a result from an RDD and either return it to the driver program or write it to a file.\n",
    "\n",
    "*Check out Spark's Programming guide for a more complete list of available methods:*\n",
    "http://spark.apache.org/docs/latest/programming-guide.html\n",
    "\n",
    "### Transformations\n",
    "Below, you will find descriptions and examples of the transformations we will use to properly format our data.  \n",
    "\n",
    "* `map(func)`: Takes in a function and applies it to each element in an RDD.\n",
    "```python\n",
    "squared = rdd.map(lambda x: x * x)\n",
    "```\n",
    "* `filter(func)`: Takes in a condition and keeps elements in the RDD that meet it.\n",
    "```python\n",
    "filtered = rdd.filter(lambda x: x > 10)\n",
    "```\n",
    "\n",
    "Now that we've seen some example transformations, we can apply these to our RDD to properly format it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'1/6/2016', u'East', u'Jones', u'Pencil', u'95', u'1.99', u'189.05'], [u'1/23/2016', u'Central', u'Kivell', u'Binder', u'50', u'19.99', u'999.50'], [u'2/9/2016', u'Central', u'Jardine', u'Pencil', u'36', u'4.99', u'179.64']]\n"
     ]
    }
   ],
   "source": [
    "list_rdd = raw_rdd.map(lambda line: [word.strip() for word in line.split(',')])\n",
    "header = list_rdd.first()\n",
    "rdd = list_rdd.filter(lambda line: line != header)\n",
    "rdd.persist() # Caches the RDD in memory to avoid disk reads. Use when frequently reusing an RDD.\n",
    "print rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have an RDD of the form `RDD[List[String]]` - something we can work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Now, let's say that we want to find the total amount of revenue made by our stationery company(remember that the total column is the last one). First, let's cover the reduce action:\n",
    "* `reduce(func)`: Takes in an aggregating function that operations on two elements and returns an element of the same type.\n",
    "```python\n",
    "rdd_sum = rdd.reduce(lambda a, b: a + b)\n",
    "```\n",
    "As well, note that `.first()` and `.take()` are both considered actions as they turn values to the driver programs.\n",
    "\n",
    "Next, let's see how we can apply reduce to our RDD to get the total sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19627.88\n"
     ]
    }
   ],
   "source": [
    "total_sales = (rdd.map(lambda line: float(line[-1]))\n",
    "              .reduce(lambda a, b: a + b))\n",
    "print total_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairRDD Transformations\n",
    "Often, we find ourselves working with data in a `(key, value)` format. Spark has built in operations for working with data like this. When an contains a Python tuple such as `(1, 2)`, these operations become available.\n",
    "* `reduceByKey(func)`: When called on a dataset of `(key, value)` pairs it returns an RDD of format `(key, aggregated values)` where the values for each key are aggregated by `func`.  \n",
    "```python\n",
    "rgg_rbk = rdd.reduceByKey(lambda a, b: a + b)\n",
    "```\n",
    "Below, we'll use `reduceByKey()` count the number of each stationery type sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Binder', 722), (u'Pen Set', 395), (u'Pen', 278), (u'Pencil', 716), (u'Desk', 10)]\n"
     ]
    }
   ],
   "source": [
    "rep_units_rdd =  rdd.map(lambda line: (line[3], int(line[4])))\n",
    "unit_sums_rdd = rep_units_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print unit_sums_rdd.collect()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with DataFrames\n",
    "When Spark 1.6 came around, Spark introduced the `DataFrames` API. `DataFrames` are organized into named columns - similar to a relational database - and provide a more structured approach to working with distributed data. (`DataSets`, which combine the functionality of `RDDs` and `DataFrames`, are not available in Spark's Python API.)\n",
    "\n",
    "`DataFrames` use  significantly less memory when caching and provide `SparkSQL` functionality. When working with `DataFrames`, a `SparkSession` is used to initalize the `SparkContext` and `SparkConf`. `SparkSessions` also allow for the use of `RDDs` - essentially, the Spark folks created them to allow for a singular entry point to a Spark program regardless of whether `RDDs` or `DataFrames` are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Learning Spark Dataframes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read in the `.csv` file, this time using the `DataFrame` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: integer (nullable = true)\n",
      " |-- Unit Cost: double (nullable = true)\n",
      " |-- Total: double (nullable = true)\n",
      "\n",
      "+----------+-------+--------+-------+-----+---------+-------+\n",
      "| OrderDate| Region|     Rep|   Item|Units|Unit Cost|  Total|\n",
      "+----------+-------+--------+-------+-----+---------+-------+\n",
      "|  1/6/2016|   East|   Jones| Pencil|   95|     1.99| 189.05|\n",
      "| 1/23/2016|Central|  Kivell| Binder|   50|    19.99|  999.5|\n",
      "|  2/9/2016|Central| Jardine| Pencil|   36|     4.99| 179.64|\n",
      "| 2/26/2016|Central|    Gill|    Pen|   27|    19.99| 539.73|\n",
      "| 3/15/2016|   West| Sorvino| Pencil|   56|     2.99| 167.44|\n",
      "|  4/1/2016|   East|   Jones| Binder|   60|     4.99|  299.4|\n",
      "| 4/18/2016|Central| Andrews| Pencil|   75|     1.99| 149.25|\n",
      "|  5/5/2016|Central| Jardine| Pencil|   90|     4.99|  449.1|\n",
      "| 5/22/2016|   West|Thompson| Pencil|   32|     1.99|  63.68|\n",
      "|  6/8/2016|   East|   Jones| Binder|   60|     8.99|  539.4|\n",
      "| 6/25/2016|Central|  Morgan| Pencil|   90|     4.99|  449.1|\n",
      "| 7/12/2016|   East|  Howard| Binder|   29|     1.99|  57.71|\n",
      "| 7/29/2016|   East|  Parent| Binder|   81|    19.99|1619.19|\n",
      "| 8/15/2016|   East|   Jones| Pencil|   35|     4.99| 174.65|\n",
      "|  9/1/2016|Central|   Smith|   Desk|    2|    125.0|  250.0|\n",
      "| 9/18/2016|   East|   Jones|Pen Set|   16|    15.99| 255.84|\n",
      "| 10/5/2016|Central|  Morgan| Binder|   28|     8.99| 251.72|\n",
      "|10/22/2016|   East|   Jones|    Pen|   64|     8.99| 575.36|\n",
      "| 11/8/2016|   East|  Parent|    Pen|   15|    19.99| 299.85|\n",
      "|11/25/2016|Central|  Kivell|Pen Set|   96|     4.99| 479.04|\n",
      "+----------+-------+--------+-------+-----+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load('sample_data/stationery.csv')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the header was properly mapped to the DataFrame columns and there was no need to split up any strings!\n",
    "### Selecting, Filtering, and GroupBy\n",
    "In Python it’s possible to access a `DataFrame’s` columns either by attribute (df.age) or by indexing (df['age']). While the former is convenient for interactive data exploration, users are highly encouraged to use the latter form, which is future proof and won’t break with column names that are also attributes on the DataFrame class.\n",
    "\n",
    "Below are two examples of how to select and filter your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Item|Units|\n",
      "+------+-----+\n",
      "|Pencil|   95|\n",
      "|Binder|   50|\n",
      "|Pencil|   36|\n",
      "|   Pen|   27|\n",
      "|Pencil|   56|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------+------+-------+-----+---------+-------+\n",
      "| OrderDate|Region|   Rep|   Item|Units|Unit Cost|  Total|\n",
      "+----------+------+------+-------+-----+---------+-------+\n",
      "|  6/8/2016|  East| Jones| Binder|   60|     8.99|  539.4|\n",
      "| 7/29/2016|  East|Parent| Binder|   81|    19.99|1619.19|\n",
      "| 9/18/2016|  East| Jones|Pen Set|   16|    15.99| 255.84|\n",
      "|10/22/2016|  East| Jones|    Pen|   64|     8.99| 575.36|\n",
      "| 11/8/2016|  East|Parent|    Pen|   15|    19.99| 299.85|\n",
      "+----------+------+------+-------+-----+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"Item\"], df[\"Units\"]).show(5)\n",
    "df.filter((df[\"Unit Cost\"] > 5) & (df[\"Region\"] == \"East\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL\n",
    "In order to leverage SparkSQL, we need to create an SQL Temporary View and then pass it an SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|     Rep|sum(Units)|\n",
      "+--------+----------+\n",
      "|   Jones|       396|\n",
      "| Jardine|       281|\n",
      "|   Smith|       156|\n",
      "| Sorvino|       142|\n",
      "|  Kivell|       193|\n",
      "|  Parent|       170|\n",
      "| Andrews|       183|\n",
      "|Thompson|        89|\n",
      "|  Morgan|       173|\n",
      "|    Gill|       213|\n",
      "|  Howard|       125|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"stationery\")\n",
    "query = \"SELECT Rep, SUM(Units) FROM stationery GROUP BY Rep\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UserDefinedFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we need to perform an action on one of the columns - something similar to the functionality of RDD's map. We use `UserDefinedFunctions` to accomplish this. With a `UserDefinedFunction`, we pass in a DataFrame column and it transforms it.\n",
    "\n",
    "Note that `udfs` are treated like a black box by Spark, meaning it doesn't try to optimitze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+-------+-----+---------+-------+----------------+\n",
      "| OrderDate| Region|     Rep|   Item|Units|Unit Cost|  Total|Multiplied Total|\n",
      "+----------+-------+--------+-------+-----+---------+-------+----------------+\n",
      "|  1/6/2016|   East|   Jones| Pencil|   95|     1.99| 189.05|          189.05|\n",
      "| 1/23/2016|Central|  Kivell| Binder|   50|    19.99|  999.5|           999.5|\n",
      "|  2/9/2016|Central| Jardine| Pencil|   36|     4.99| 179.64|          179.64|\n",
      "| 2/26/2016|Central|    Gill|    Pen|   27|    19.99| 539.73|          539.73|\n",
      "| 3/15/2016|   West| Sorvino| Pencil|   56|     2.99| 167.44|          167.44|\n",
      "|  4/1/2016|   East|   Jones| Binder|   60|     4.99|  299.4|           299.4|\n",
      "| 4/18/2016|Central| Andrews| Pencil|   75|     1.99| 149.25|          149.25|\n",
      "|  5/5/2016|Central| Jardine| Pencil|   90|     4.99|  449.1|           449.1|\n",
      "| 5/22/2016|   West|Thompson| Pencil|   32|     1.99|  63.68|           63.68|\n",
      "|  6/8/2016|   East|   Jones| Binder|   60|     8.99|  539.4|           539.4|\n",
      "| 6/25/2016|Central|  Morgan| Pencil|   90|     4.99|  449.1|           449.1|\n",
      "| 7/12/2016|   East|  Howard| Binder|   29|     1.99|  57.71|           57.71|\n",
      "| 7/29/2016|   East|  Parent| Binder|   81|    19.99|1619.19|         1619.19|\n",
      "| 8/15/2016|   East|   Jones| Pencil|   35|     4.99| 174.65|          174.65|\n",
      "|  9/1/2016|Central|   Smith|   Desk|    2|    125.0|  250.0|           250.0|\n",
      "| 9/18/2016|   East|   Jones|Pen Set|   16|    15.99| 255.84|          255.84|\n",
      "| 10/5/2016|Central|  Morgan| Binder|   28|     8.99| 251.72|          251.72|\n",
      "|10/22/2016|   East|   Jones|    Pen|   64|     8.99| 575.36|          575.36|\n",
      "| 11/8/2016|   East|  Parent|    Pen|   15|    19.99| 299.85|          299.85|\n",
      "|11/25/2016|Central|  Kivell|Pen Set|   96|     4.99| 479.04|          479.04|\n",
      "+----------+-------+--------+-------+-----+---------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def multiplied(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiplied_udf = udf(multiplied, FloatType())\n",
    "multiplied_df = df.withColumn(\"Multiplied Total\", multiplied_udf(df[\"Units\"], df[\"Unit Cost\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here?\n",
    "\n",
    "We've only scratched the surface of what Spark is capable of. I would turn to the Spark Programming Guide at http://spark.apache.org/docs/latest/programming-guide.html and review the guides for each of the libraries in Spark. To directly build upon what you've learned here, look into:\n",
    "\n",
    "* Pseudo Set Operations like `rdd.distinct()`, `rdd.union()`, `rdd.intersection()`, etc.\n",
    "* Additional transformations like `flatMap()` and `aggregate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
